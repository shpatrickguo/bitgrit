{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6bcf83f-9028-4800-9c13-73b52e9df193",
   "metadata": {},
   "source": [
    "# Viral Tweets Prediction Challenge\n",
    "Develop a machine learning model to predict the virality level of each tweet based on attributes such as tweet content, media attached to the tweet, and date/time published."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737dbf65-ae41-43cc-9789-14740cf77f14",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86d61f5-34b3-4e15-a8d4-8c3830d12017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#building models\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743b1d5-0369-4e58-9d70-30c41de2b04a",
   "metadata": {},
   "source": [
    "Data reading with memory footprint reduction. Implementation copied from [Eryk Lewson](https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132b1dc1-1693-4772-8763-e0876d57471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes the minimum and the maximum of each column and changes the data type to what is optimal for the column.\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be6fd5-b807-43ee-acda-1851218646a0",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c89c61-6cad-43e7-9fb0-895acb82b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#final_df = reduce_mem_usage(pd.read_csv(\"s3://daanmatchdatafiles/bitgrit/final_df.csv\"))\n",
    "#p_final_df = reduce_mem_usage(pd.read_csv(\"s3://daanmatchdatafiles/bitgrit/p_final_df.csv\"))\n",
    "final_df = reduce_mem_usage(pd.read_csv(\"final_df.csv\"))\n",
    "p_final_df = reduce_mem_usage(pd.read_csv(\"p_final_df.csv\"))\n",
    "print(\"Shape of train set: \", final_df.shape)\n",
    "print(\"Shape of test set: \", p_final_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8e322-9834-495d-91c9-b4895fc93e13",
   "metadata": {},
   "source": [
    "# Model fitting with HyperParameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b56ca0-b420-4acb-bc1d-822516bb2ec3",
   "metadata": {},
   "source": [
    "## Split the full sample into train/test (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfb4e03-76d7-48ed-a3f7-4b5e6c3ddf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape  (20737, 2944)\n",
      "Test set shape  (8888, 2944)\n"
     ]
    }
   ],
   "source": [
    "X = final_df.drop(['virality', 'tweet_user_id', 'tweet_id', 'user_id'], axis=1)\n",
    "y = final_df['virality']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "print('Training set shape ', X_train.shape)\n",
    "print('Test set shape ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0e4a2-8982-4e18-9d6a-03ef245c22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c7995dc-2e36-47b8-8ad7-10d6efcf924c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.003, max_bin=200, max_depth=10, num_leaves=150)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.set_params(**params)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c664880-f372-4e21-a2c1-d3e73440fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cf81d-2b9a-4323-bd59-9c9d88bb4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Base accuracy 66.45%\n",
    "print('Accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc623b06-5f84-4d0f-8a98-d11bd3bd5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature importance\n",
    "feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:10], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a58590-b35a-46f2-aec4-6c5c189e0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb4846-b02e-4f32-9c7c-d87aad12f11e",
   "metadata": {},
   "source": [
    "Use density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08234147-b3d0-4864-b8de-c3d7945af1ac",
   "metadata": {},
   "source": [
    "# Fit model to Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd960310-31d5-4dec-b710-3a225c6486d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = p_final_df.drop(['tweet_user_id', 'tweet_id', 'user_id'], axis=1)\n",
    "\n",
    "solution = clf.predict(X)\n",
    "solution_df = pd.concat([p_final_df[['tweet_id']], pd.DataFrame(solution, columns = ['virality'])], axis=1)\n",
    "solution_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b63271-0116-43d8-9cef-3837c1d3a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution_df.to_csv('solution.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b243049-a030-48a3-bcbd-0a94e315a054",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad330df-b4cd-4b54-8dc1-5d4a11ab8876",
   "metadata": {},
   "source": [
    "- More feature engineering\n",
    "- Further parameter tuning\n",
    "- Stacking ensemble ML models\n",
    "- Early stopping in LightGBM model training to avoid overtraining\n",
    "- Learning rate decay in LightGBM model training to improve convergence to the minimum\n",
    "- Hyperparameter optimisation of the model using random search in cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c9955-a469-479c-a1e7-2303d1f38ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
