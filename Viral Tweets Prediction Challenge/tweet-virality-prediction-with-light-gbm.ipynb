{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Viral Tweets Prediction Challenge\nDevelop a machine learning model to predict the virality level of each tweet based on attributes such as tweet content, media attached to the tweet, and date/time published.","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport numpy as np\nimport time\nimport timeit\nimport collections\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n\n# Preprocessing + Feature Selection\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\n# Model Building\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n# Hyperparameter tuning \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\nimport shap","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:00:56.717879Z","iopub.execute_input":"2021-06-25T22:00:56.718546Z","iopub.status.idle":"2021-06-25T22:01:08.51755Z","shell.execute_reply.started":"2021-06-25T22:00:56.718422Z","shell.execute_reply":"2021-06-25T22:01:08.516803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function takes the minimum and the maximum of each column and changes the data type to what is optimal for the column.\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:08.519412Z","iopub.execute_input":"2021-06-25T22:01:08.520021Z","iopub.status.idle":"2021-06-25T22:01:08.534244Z","shell.execute_reply.started":"2021-06-25T22:01:08.519974Z","shell.execute_reply":"2021-06-25T22:01:08.533155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data retrieval","metadata":{}},{"cell_type":"code","source":"# Kagge storage path\npath = \"../input/viral-tweets/Dataset/\"\n\n# Training datasets\ntrain_tweets = pd.read_csv(path + 'Tweets/train_tweets.csv')\ntrain_tweets_vectorized_media = pd.read_csv(path + 'Tweets/train_tweets_vectorized_media.csv')\ntrain_tweets_vectorized_text = pd.read_csv(path + 'Tweets/train_tweets_vectorized_text.csv')\n\n# Test dataset\ntest_tweets = pd.read_csv(path + 'Tweets/test_tweets.csv')\ntest_tweets_vectorized_media = pd.read_csv(path + 'Tweets/test_tweets_vectorized_media.csv')\ntest_tweets_vectorized_text = pd.read_csv(path + 'Tweets/test_tweets_vectorized_text.csv')\n\n# User dataset\nusers = pd.read_csv(path + 'Users/users.csv')\nuser_vectorized_descriptions = pd.read_csv(path + 'Users/user_vectorized_descriptions.csv')\nuser_vectorized_profile_images = pd.read_csv(path + 'Users/user_vectorized_profile_images.csv')\n\n# Solutions format\nsolutions_format = pd.read_csv(path + \"solution_format.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:08.536926Z","iopub.execute_input":"2021-06-25T22:01:08.537368Z","iopub.status.idle":"2021-06-25T22:01:49.472024Z","shell.execute_reply.started":"2021-06-25T22:01:08.53731Z","shell.execute_reply":"2021-06-25T22:01:49.470604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensions of Data","metadata":{}},{"cell_type":"code","source":"# print dimensions of data\nprint('Dimensions:')\nprint('Train tweets:', train_tweets.shape)\nprint('Train tweets vectorized media:', train_tweets_vectorized_media.shape)\nprint('Train tweets vectorized text:', train_tweets_vectorized_text.shape)\nprint()\n\nprint('Test tweets:', test_tweets.shape)\nprint('Test tweets vectorized media:', test_tweets_vectorized_media.shape)\nprint('Test tweets vectorized text:', test_tweets_vectorized_text.shape)\nprint()\n\nprint('Users:', users.shape)\nprint('User vectorized descriptions:', user_vectorized_descriptions.shape)\nprint('User vectorized profile images:', user_vectorized_profile_images.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:49.474155Z","iopub.execute_input":"2021-06-25T22:01:49.47461Z","iopub.status.idle":"2021-06-25T22:01:49.486343Z","shell.execute_reply.started":"2021-06-25T22:01:49.474561Z","shell.execute_reply":"2021-06-25T22:01:49.484783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dimensions for ```Users``` are smaller than ```Tweets```, which indicate that the users in the dataset may have multiple tweets.    \nVectorized text has the same number of rows as tweets, meaning that all tweets have text.  \nVectorized media has fewer rows than tweets, indicating that not all tweets have media or that some tweets have multiple media.  \nAll ```Users``` have descriptions and profile images.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n## Train Tweets","metadata":{}},{"cell_type":"code","source":"train_tweets.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:49.488224Z","iopub.execute_input":"2021-06-25T22:01:49.488774Z","iopub.status.idle":"2021-06-25T22:01:49.526865Z","shell.execute_reply.started":"2021-06-25T22:01:49.488718Z","shell.execute_reply":"2021-06-25T22:01:49.525783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primary Keys: ```tweet_id```, ```tweet_user_id```. There are 11 Features. Target variable: ```virality```. Tweet data are connected thorugh ```tweet_id```.","metadata":{}},{"cell_type":"code","source":"train_tweets.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:49.52809Z","iopub.execute_input":"2021-06-25T22:01:49.528407Z","iopub.status.idle":"2021-06-25T22:01:49.559567Z","shell.execute_reply.started":"2021-06-25T22:01:49.528375Z","shell.execute_reply":"2021-06-25T22:01:49.558236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tweet Creation Date\ntweet_created_at_year  \ntweet_created_at_day  \ntweet_created_at_month  \ntweet_created_at_hour","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(12, 8))\nsns.histplot(train_tweets, x = 'tweet_created_at_year', discrete = True, ax = axs[0,0])\nsns.histplot(train_tweets, x = 'tweet_created_at_day', discrete = True, ax = axs[0,1])\nsns.histplot(train_tweets, x = 'tweet_created_at_month', discrete = True, ax = axs[1,0])\nsns.histplot(train_tweets, x = 'tweet_created_at_hour', discrete = True, ax = axs[1,1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:49.560933Z","iopub.execute_input":"2021-06-25T22:01:49.561217Z","iopub.status.idle":"2021-06-25T22:01:50.539059Z","shell.execute_reply.started":"2021-06-25T22:01:49.561189Z","shell.execute_reply":"2021-06-25T22:01:50.537911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The histplot for ```tweet_created_at_year``` shows a left skeweed distribution between 2013-2020 where each subsequent year has more tweets created. Note that this data was produced during 2021, so the count for ```tweet_created_at_year``` for 2021 does not account for the full year unlike others.\n- The histplot for ```tweet_created_at_month``` show that December is the month with the highest number of tweets created. The lowest being March.\n- The histplot for ```tweet_created_at_day``` generally has a uniform distribution. The highest being 27th, perhaps because February have 28 days. The 31st is an outlier because not all months have 31 days. \n- The histplot for ```tweet_created_at_hour``` show a cyclical distribution where most tweets are created during the afternoon/evening, the highest being 4pm. The least amount where created late at night/early in the morning.","metadata":{}},{"cell_type":"markdown","source":"### Tweet Message Content\ntweet_hashtag_count  \ntweet_url_count  \ntweet_mention_count","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3, 1, figsize=(12, 12))\nsns.histplot(x = 'tweet_hashtag_count', data = train_tweets, discrete = True, ax = axs[0])\nsns.histplot(x = 'tweet_url_count', data = train_tweets, discrete = True, ax = axs[1])\nsns.histplot(x = 'tweet_mention_count', data = train_tweets, discrete = True, ax = axs[2])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:50.540567Z","iopub.execute_input":"2021-06-25T22:01:50.540886Z","iopub.status.idle":"2021-06-25T22:01:51.083238Z","shell.execute_reply.started":"2021-06-25T22:01:50.540853Z","shell.execute_reply":"2021-06-25T22:01:51.081806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The histplot for ```tweet_hashtag_count``` is right skewed where most tweets have zero hashtags and less tweets have more hashtags.\n- The histplot for ```tweet_url_count``` shows that most tweets have one URL, and not many tweets have a high number of tweets.\n- The histplot for ```tweet_mention_count``` is right skewed where most tweets have zero mentions and less tweets have multiple hashtags.","metadata":{}},{"cell_type":"markdown","source":"### Tweet Attatchment\ntweet_has_attachment   \ntweet_attachment_class","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 1, figsize=(10, 10))\nsns.countplot(x = 'tweet_has_attachment', data = train_tweets, ax = axs[0])\nsns.countplot(x = 'tweet_attachment_class', data = train_tweets, ax = axs[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:51.087117Z","iopub.execute_input":"2021-06-25T22:01:51.087445Z","iopub.status.idle":"2021-06-25T22:01:51.370377Z","shell.execute_reply.started":"2021-06-25T22:01:51.087414Z","shell.execute_reply":"2021-06-25T22:01:51.368845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The countplot for ```tweet_has_attachment``` shows that more tweets have an attachment, such as media.\n- The countplot for ```tweet_attachment_class``` shows that most tweets have an attachment class A, and very few tweets have attachment class B.","metadata":{}},{"cell_type":"markdown","source":"### Tweet Language","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 1, figsize=(8, 3))\nsns.countplot(x = 'tweet_language_id', data = train_tweets, ax = axs)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:51.372417Z","iopub.execute_input":"2021-06-25T22:01:51.372753Z","iopub.status.idle":"2021-06-25T22:01:51.691258Z","shell.execute_reply.started":"2021-06-25T22:01:51.372721Z","shell.execute_reply":"2021-06-25T22:01:51.690232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The countplot for ```tweet_language_id``` shows a high amount of tweets in language_id 0, which is presumed to be english. Very few tweets in this datset are in other languages.","metadata":{}},{"cell_type":"markdown","source":"### Tweet Virality","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = 'virality', data = train_tweets)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:51.692534Z","iopub.execute_input":"2021-06-25T22:01:51.692878Z","iopub.status.idle":"2021-06-25T22:01:51.834996Z","shell.execute_reply.started":"2021-06-25T22:01:51.692844Z","shell.execute_reply":"2021-06-25T22:01:51.833841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The countplot for ```virality``` shows the virality of tweets where 1 is low whereas 5 is high. Most tweets have a virality of 1.\n\nSince there are 5 values in ```virality```, this means that this is a multi-class classification problem.","metadata":{}},{"cell_type":"markdown","source":"### Correlation Matrix","metadata":{}},{"cell_type":"code","source":"corrmat = train_tweets.corr()[2:] \nsns.heatmap(corrmat, square=True);","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:51.837489Z","iopub.execute_input":"2021-06-25T22:01:51.837854Z","iopub.status.idle":"2021-06-25T22:01:52.210363Z","shell.execute_reply.started":"2021-06-25T22:01:51.837818Z","shell.execute_reply":"2021-06-25T22:01:52.209531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The heatmap shows that some features have correlation with each other. ```tweet_url_count``` and ```tweet_has_attachment``` has the highest correlation with each other.","metadata":{}},{"cell_type":"code","source":"df_corr = train_tweets.corr()['virality'][2:-1]\ntop_features = df_corr.sort_values(ascending=False, key=abs)\ntop_features","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:52.211825Z","iopub.execute_input":"2021-06-25T22:01:52.212408Z","iopub.status.idle":"2021-06-25T22:01:52.236804Z","shell.execute_reply.started":"2021-06-25T22:01:52.212364Z","shell.execute_reply":"2021-06-25T22:01:52.235731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation numbers show a low correlation between virality and features, meaning they cannot be used linearly to predict virality.","metadata":{}},{"cell_type":"markdown","source":"## Train Tweets Vecotrized Media","metadata":{}},{"cell_type":"code","source":"train_tweets_vectorized_media.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:52.238054Z","iopub.execute_input":"2021-06-25T22:01:52.238367Z","iopub.status.idle":"2021-06-25T22:01:52.349634Z","shell.execute_reply.started":"2021-06-25T22:01:52.238337Z","shell.execute_reply":"2021-06-25T22:01:52.348491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primary Keys: ```media_id```, ```tweet_id```. There are 2048 Features. Tweet data are connected thorugh ```tweet_id```.","metadata":{}},{"cell_type":"markdown","source":"## Train Tweets Vectorized Text","metadata":{}},{"cell_type":"code","source":"train_tweets_vectorized_text.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:52.351481Z","iopub.execute_input":"2021-06-25T22:01:52.352098Z","iopub.status.idle":"2021-06-25T22:01:52.402316Z","shell.execute_reply.started":"2021-06-25T22:01:52.352042Z","shell.execute_reply":"2021-06-25T22:01:52.400989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primary Keys: ```tweet_id```. There are 768 Features. Tweet data are connected thorugh ```tweet_id```.\n\nEach column in Vectorized Text/Media represents one coordinate in the numeric feature space","metadata":{}},{"cell_type":"markdown","source":"## Users","metadata":{}},{"cell_type":"code","source":"users.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:52.403591Z","iopub.execute_input":"2021-06-25T22:01:52.403924Z","iopub.status.idle":"2021-06-25T22:01:52.418849Z","shell.execute_reply.started":"2021-06-25T22:01:52.403893Z","shell.execute_reply":"2021-06-25T22:01:52.417664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primary Keys: ```user_id```. There are 10 Features. User data are connected thorugh ```user_id```.","metadata":{}},{"cell_type":"markdown","source":"### User Count\nuser_like_count  \nuser_followers_count  \nuser_following_count  \nuser_listed_on_count  \nuser_tweet_count","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize=(18, 8))\nsns.histplot(users, x = 'user_like_count', ax = axs[0,0])\nsns.histplot(users, x = 'user_followers_count', ax = axs[0,1])\nsns.histplot(users, x = 'user_following_count', ax = axs[0,2])\nsns.histplot(users, x = 'user_listed_on_count', ax = axs[1,0])\nsns.histplot(users, x = 'user_tweet_count', ax = axs[1,1])\naxs[1][2].set_visible(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:52.42029Z","iopub.execute_input":"2021-06-25T22:01:52.420597Z","iopub.status.idle":"2021-06-25T22:01:53.683864Z","shell.execute_reply.started":"2021-06-25T22:01:52.420566Z","shell.execute_reply":"2021-06-25T22:01:53.682719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The histplot for ```user_like_count``` is right skewed. A large propotion of users have between 0-2500 likes.\n- The histplot for ```user_follower_count``` is right skewed. A large propotion of users have between 0-10000 followers.\n- The histplot for ```user_following_count``` is right skewed. A large propotion of users follow between 0-1000 accounts.\n- The histplot for ```user_listed_on_count``` is right skewed. A large propotion of users are listed on between 0-5000 lists.\n- The histplot for ```user_tweet_count``` is right skewed. A large propotion of users have between 0-10000 tweeets.","metadata":{}},{"cell_type":"markdown","source":"### User Creation Date","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 1, figsize=(12, 8))\nsns.histplot(users, x = 'user_created_at_year', discrete = True, ax = axs[0])\nsns.histplot(users, x = 'user_created_at_month', discrete = True, ax = axs[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:53.685178Z","iopub.execute_input":"2021-06-25T22:01:53.685514Z","iopub.status.idle":"2021-06-25T22:01:54.020682Z","shell.execute_reply.started":"2021-06-25T22:01:53.685483Z","shell.execute_reply":"2021-06-25T22:01:54.019562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The histplot for ```user_created_at_year``` shows that most users were created in 2011.\n- The histplot for ```user_created_at_month``` shows that most users were created in August. 0 users were creaed in March, which may explain why March has the lowest tweets created.","metadata":{}},{"cell_type":"markdown","source":"### User Has\nuser_has_location  \nuser_has_url  \nuser_verified","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(16, 6))\nsns.countplot(x = 'user_has_location', data = users, ax = axs[0])\nsns.countplot(x = 'user_has_url', data = users, ax = axs[1])\nsns.countplot(x = 'user_verified', data = users, ax = axs[2])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:54.022002Z","iopub.execute_input":"2021-06-25T22:01:54.022398Z","iopub.status.idle":"2021-06-25T22:01:54.624676Z","shell.execute_reply.started":"2021-06-25T22:01:54.022363Z","shell.execute_reply":"2021-06-25T22:01:54.62355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the binary data: most of the users have their location and url listed on their accounts. Most of them are not verified.","metadata":{}},{"cell_type":"markdown","source":"## User Vectorized Descriptions","metadata":{}},{"cell_type":"code","source":"user_vectorized_descriptions.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:54.625963Z","iopub.execute_input":"2021-06-25T22:01:54.626253Z","iopub.status.idle":"2021-06-25T22:01:54.675533Z","shell.execute_reply.started":"2021-06-25T22:01:54.626225Z","shell.execute_reply":"2021-06-25T22:01:54.674511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primary Keys: ```user_id```. There are 768 Features. User data are connected thorugh ```user_id```.\n\nVectorized descriptions and vectorized text have the same number of features.","metadata":{}},{"cell_type":"markdown","source":"## User Vectorized Profile Images","metadata":{}},{"cell_type":"code","source":"user_vectorized_profile_images.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:54.677863Z","iopub.execute_input":"2021-06-25T22:01:54.678348Z","iopub.status.idle":"2021-06-25T22:01:54.787408Z","shell.execute_reply.started":"2021-06-25T22:01:54.6783Z","shell.execute_reply":"2021-06-25T22:01:54.78627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primary Keys: ```user_id```. There are 2048 Features. User data are connected thorugh ```user_id```.\n\nVectorized media and vectorized profile images have the same number of features.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing & Wrangling","metadata":{}},{"cell_type":"code","source":"train_tweets.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:54.788783Z","iopub.execute_input":"2021-06-25T22:01:54.789088Z","iopub.status.idle":"2021-06-25T22:01:54.805895Z","shell.execute_reply.started":"2021-06-25T22:01:54.789058Z","shell.execute_reply":"2021-06-25T22:01:54.804688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only ```tweet_topic_ids``` have null values. These will treated as another tweet_topic_id by filling them with another id such as ```[\"0\"]```. (The number does not matter as long as it is distinct from other values).","metadata":{}},{"cell_type":"code","source":"train_tweets.fillna({'tweet_topic_ids':\"['0']\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:54.807554Z","iopub.execute_input":"2021-06-25T22:01:54.808001Z","iopub.status.idle":"2021-06-25T22:01:54.816527Z","shell.execute_reply.started":"2021-06-25T22:01:54.807965Z","shell.execute_reply":"2021-06-25T22:01:54.815722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rest of the data files do not have non-null values.","metadata":{"execution":{"iopub.status.busy":"2021-06-24T20:38:21.728025Z","iopub.execute_input":"2021-06-24T20:38:21.728422Z","iopub.status.idle":"2021-06-24T20:38:22.517514Z","shell.execute_reply.started":"2021-06-24T20:38:21.728388Z","shell.execute_reply":"2021-06-24T20:38:22.516478Z"}}},{"cell_type":"markdown","source":"## Categorical Variables\n### Train Tweets\n#### One-hot encoding","metadata":{}},{"cell_type":"code","source":"# Split topic ids\ntopic_ids = (\n    train_tweets.tweet_topic_ids.str.strip('[]').str.split('\\s*,\\s*').explode().str.get_dummies().sum(level=0).add_prefix('topic_id_')\n) \ntopic_ids.rename(columns = lambda x: x.replace(\"'\", \"\"), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:01:54.818219Z","iopub.execute_input":"2021-06-25T22:01:54.818643Z","iopub.status.idle":"2021-06-25T22:02:06.355914Z","shell.execute_reply.started":"2021-06-25T22:01:54.818604Z","shell.execute_reply":"2021-06-25T22:02:06.355025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year = pd.get_dummies(train_tweets.tweet_created_at_year, prefix='year')\nmonth = pd.get_dummies(train_tweets.tweet_created_at_month , prefix='month')\nday = pd.get_dummies(train_tweets.tweet_created_at_day, prefix='day')\nattachment = pd.get_dummies(train_tweets.tweet_attachment_class, prefix='attatchment')\nlanguage = pd.get_dummies(train_tweets.tweet_language_id, prefix='language')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.357127Z","iopub.execute_input":"2021-06-25T22:02:06.357655Z","iopub.status.idle":"2021-06-25T22:02:06.373206Z","shell.execute_reply.started":"2021-06-25T22:02:06.35759Z","shell.execute_reply":"2021-06-25T22:02:06.371943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cyclical Encoding\nFrom histplot we saw that hours have a cyclical distribution so we will us cyclical encoding.","metadata":{}},{"cell_type":"code","source":"hour_sin = np.sin(2 * np.pi * train_tweets['tweet_created_at_hour']/24.0)\nhour_sin.name = 'hour_sin'\nhour_cos = np.cos(2 * np.pi * train_tweets['tweet_created_at_hour']/24.0)\nhour_cos.name = 'hour_cos'","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.374669Z","iopub.execute_input":"2021-06-25T22:02:06.375051Z","iopub.status.idle":"2021-06-25T22:02:06.386507Z","shell.execute_reply.started":"2021-06-25T22:02:06.375019Z","shell.execute_reply":"2021-06-25T22:02:06.385345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join encoded data to train data.\ncolumns_drop = [\n                \"tweet_topic_ids\",\n                \"tweet_created_at_year\",\n                \"tweet_created_at_month\",\n                \"tweet_created_at_day\",\n                \"tweet_attachment_class\",\n                \"tweet_language_id\",\n                \"tweet_created_at_hour\",\n               ]\nencoded = [topic_ids, year, month, day, attachment, language, hour_sin, hour_cos]\n\ntrain_tweets_final = train_tweets.drop(columns_drop, 1).join(encoded)\ntrain_tweets_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.393379Z","iopub.execute_input":"2021-06-25T22:02:06.39402Z","iopub.status.idle":"2021-06-25T22:02:06.444102Z","shell.execute_reply.started":"2021-06-25T22:02:06.393968Z","shell.execute_reply":"2021-06-25T22:02:06.442982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Users\n#### One-hot encoding","metadata":{}},{"cell_type":"code","source":"year = pd.get_dummies(users.user_created_at_year, prefix='year')\nmonth = pd.get_dummies(users.user_created_at_month , prefix='month')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.446538Z","iopub.execute_input":"2021-06-25T22:02:06.446873Z","iopub.status.idle":"2021-06-25T22:02:06.453803Z","shell.execute_reply.started":"2021-06-25T22:02:06.446834Z","shell.execute_reply":"2021-06-25T22:02:06.452799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join encoded data to train data.\ncolumns_drop = [\n                \"user_created_at_year\",\n                \"user_created_at_month\",\n               ]\ndfs = [year, month]\n\nusers_final = users.drop(columns_drop, 1).join(dfs)\nusers_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.4551Z","iopub.execute_input":"2021-06-25T22:02:06.455411Z","iopub.status.idle":"2021-06-25T22:02:06.488909Z","shell.execute_reply.started":"2021-06-25T22:02:06.455382Z","shell.execute_reply":"2021-06-25T22:02:06.487862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize Data\nMachine learning algorithms perform better or converage faster when the features are on a small scale. Let's normalize the counts.\n### Train Tweets","metadata":{}},{"cell_type":"code","source":"# Normalize using reprocessing.normalize\nscaled_tweet_hashtag_count = preprocessing.normalize([train_tweets_final[\"tweet_hashtag_count\"]])\ntrain_tweets_final[\"tweet_hashtag_count\"] = scaled_tweet_hashtag_count[0]\n\nscaled_tweet_url_count = preprocessing.normalize([train_tweets_final[\"tweet_url_count\"]])\ntrain_tweets_final[\"tweet_url_count\"] = scaled_tweet_url_count[0]\n\nscaled_tweet_mention_count = preprocessing.normalize([train_tweets_final[\"tweet_mention_count\"]])\ntrain_tweets_final[\"tweet_mention_count\"] = scaled_tweet_mention_count[0]\ntrain_tweets_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.490488Z","iopub.execute_input":"2021-06-25T22:02:06.490917Z","iopub.status.idle":"2021-06-25T22:02:06.524034Z","shell.execute_reply.started":"2021-06-25T22:02:06.49087Z","shell.execute_reply":"2021-06-25T22:02:06.5229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### User","metadata":{}},{"cell_type":"code","source":"users_final[\"user_like_count\"] = preprocessing.normalize([users_final[\"user_like_count\"]])[0]\nusers_final[\"user_followers_count\"] = preprocessing.normalize([users_final[\"user_followers_count\"]])[0]\nusers_final[\"user_following_count\"] = preprocessing.normalize([users_final[\"user_following_count\"]])[0]\nusers_final[\"user_listed_on_count\"] = preprocessing.normalize([users_final[\"user_listed_on_count\"]])[0]\nusers_final[\"user_tweet_count\"] = preprocessing.normalize([users_final[\"user_tweet_count\"]])[0]\nusers_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.525563Z","iopub.execute_input":"2021-06-25T22:02:06.526Z","iopub.status.idle":"2021-06-25T22:02:06.560492Z","shell.execute_reply.started":"2021-06-25T22:02:06.525955Z","shell.execute_reply":"2021-06-25T22:02:06.559299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\nUse lasso regression for feature selection.\n## Train Tweets Media","metadata":{}},{"cell_type":"code","source":"print(\"train_tweets shape:\", train_tweets.shape)\nprint(\"train_tweets_vectorized_media shape:\", train_tweets_vectorized_media.shape)\n\n# Match row number between train tweets and vectorized media\nvectorized_media_df = pd.merge(train_tweets, train_tweets_vectorized_media, on='tweet_id', how='right')\n# Drop extra columns\nvectorized_media_df.drop(train_tweets.columns.difference(['virality']), axis=1, inplace=True)\nvectorized_media_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:06.561852Z","iopub.execute_input":"2021-06-25T22:02:06.562155Z","iopub.status.idle":"2021-06-25T22:02:07.368255Z","shell.execute_reply.started":"2021-06-25T22:02:06.562124Z","shell.execute_reply":"2021-06-25T22:02:07.36701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the target as well as dependent variables from image data.\ny = vectorized_media_df['virality']\nx = vectorized_media_df.loc[:, vectorized_media_df.columns.str.contains(\"img_\")] \n\n# Run Lasso regression for feature selection.\nsel_model = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear'))\n\n# time the model fitting\nstart = timeit.default_timer()\n\n# Fit the trained model on our data\nsel_model.fit(x, y)\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) \n\n# get index of good features\nsel_index = sel_model.get_support()\n\n# count the no of columns selected\ncounter = collections.Counter(sel_model.get_support())\ncounter","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:02:07.369995Z","iopub.execute_input":"2021-06-25T22:02:07.370447Z","iopub.status.idle":"2021-06-25T22:04:01.940954Z","shell.execute_reply.started":"2021-06-25T22:02:07.370398Z","shell.execute_reply":"2021-06-25T22:04:01.939729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"media_ind_df = pd.DataFrame(x[x.columns[(sel_index)]])\ntrain_tweets_media_final = pd.concat([train_tweets_vectorized_media[['media_id', 'tweet_id']], media_ind_df], axis=1)\ntrain_tweets_media_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:04:01.943048Z","iopub.execute_input":"2021-06-25T22:04:01.943567Z","iopub.status.idle":"2021-06-25T22:04:02.245207Z","shell.execute_reply.started":"2021-06-25T22:04:01.943516Z","shell.execute_reply":"2021-06-25T22:04:02.244304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Tweets Text","metadata":{}},{"cell_type":"code","source":"print(\"train_tweets shape:\", train_tweets.shape)\nprint(\"train_tweets_vectorized_text:\", train_tweets_vectorized_media.shape)\n\n# Match row number between train tweets and vectorized text\nvectorized_text_df = pd.merge(train_tweets, train_tweets_vectorized_text, on='tweet_id', how='right')\n# Drop extra columns\nvectorized_text_df.drop(train_tweets.columns.difference(['virality']), axis=1, inplace=True)\nvectorized_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:04:02.246553Z","iopub.execute_input":"2021-06-25T22:04:02.246857Z","iopub.status.idle":"2021-06-25T22:04:02.678961Z","shell.execute_reply.started":"2021-06-25T22:04:02.24683Z","shell.execute_reply":"2021-06-25T22:04:02.677859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the target as well as dependent variables from image data.\ny = vectorized_text_df['virality']\nx = vectorized_text_df.loc[:, train_tweets_vectorized_text.columns.str.contains(\"feature_\")] \n\n# time the model fitting\nstart = timeit.default_timer()\n\n# Fit the trained model on our data\nsel_model.fit(x, y)\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) \n\n# get index of good features\nsel_index = sel_model.get_support()\n\n# count the no of columns selected\ncounter = collections.Counter(sel_model.get_support())\ncounter","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:04:02.68027Z","iopub.execute_input":"2021-06-25T22:04:02.680553Z","iopub.status.idle":"2021-06-25T22:09:58.126326Z","shell.execute_reply.started":"2021-06-25T22:04:02.680526Z","shell.execute_reply":"2021-06-25T22:09:58.125316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_ind_df = pd.DataFrame(x[x.columns[(sel_index)]])\ntrain_tweets_text_final = pd.concat([train_tweets_vectorized_text[['tweet_id']], text_ind_df], axis=1)\ntrain_tweets_text_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.127811Z","iopub.execute_input":"2021-06-25T22:09:58.12815Z","iopub.status.idle":"2021-06-25T22:09:58.278257Z","shell.execute_reply.started":"2021-06-25T22:09:58.12812Z","shell.execute_reply":"2021-06-25T22:09:58.277214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## User Descriptions","metadata":{}},{"cell_type":"code","source":"# Find the median virality for each user to reduce features\naverage_virality_df = train_tweets.groupby('tweet_user_id').agg(pd.Series.median)['virality']","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.279807Z","iopub.execute_input":"2021-06-25T22:09:58.280217Z","iopub.status.idle":"2021-06-25T22:09:58.449037Z","shell.execute_reply.started":"2021-06-25T22:09:58.280172Z","shell.execute_reply":"2021-06-25T22:09:58.448013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtain median of virality since each user may have multiple tweets.","metadata":{}},{"cell_type":"code","source":"descriptions_df = pd.merge(average_virality_df, user_vectorized_descriptions, left_on='tweet_user_id', right_on='user_id', how='right')\ndescriptions_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.450533Z","iopub.execute_input":"2021-06-25T22:09:58.450867Z","iopub.status.idle":"2021-06-25T22:09:58.484405Z","shell.execute_reply.started":"2021-06-25T22:09:58.450829Z","shell.execute_reply":"2021-06-25T22:09:58.483635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the target as well as dependent variables from image data.\ny = descriptions_df['virality']\nx = descriptions_df.loc[:, descriptions_df.columns.str.contains(\"feature_\")] \n\n# time the model fitting\nstart = timeit.default_timer()\n\n# Fit the trained model on our data\nsel_model.fit(x, y)\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) \n\n# get index of good features\nsel_index = sel_model.get_support()\n\n# count the no of columns selected\ncounter = collections.Counter(sel_model.get_support())\ncounter","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.485519Z","iopub.execute_input":"2021-06-25T22:09:58.485989Z","iopub.status.idle":"2021-06-25T22:09:58.512387Z","shell.execute_reply.started":"2021-06-25T22:09:58.485953Z","shell.execute_reply":"2021-06-25T22:09:58.51132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"desc_ind_df = pd.DataFrame(x[x.columns[(sel_index)]])\nuser_descriptions_final = pd.concat([user_vectorized_descriptions[['user_id']], desc_ind_df], axis=1)\nuser_descriptions_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.513688Z","iopub.execute_input":"2021-06-25T22:09:58.513996Z","iopub.status.idle":"2021-06-25T22:09:58.527195Z","shell.execute_reply.started":"2021-06-25T22:09:58.513968Z","shell.execute_reply":"2021-06-25T22:09:58.525952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## User Profile Images","metadata":{}},{"cell_type":"code","source":"profile_images_df = pd.merge(average_virality_df, user_vectorized_profile_images, left_on='tweet_user_id', right_on='user_id', how='right')\nprofile_images_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.528558Z","iopub.execute_input":"2021-06-25T22:09:58.52888Z","iopub.status.idle":"2021-06-25T22:09:58.569365Z","shell.execute_reply.started":"2021-06-25T22:09:58.528851Z","shell.execute_reply":"2021-06-25T22:09:58.568252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the target as well as dependent variables from image data.\ny = profile_images_df['virality']\nx = profile_images_df.loc[:, profile_images_df.columns.str.contains(\"feature_\")] \n\n# time the model fitting\nstart = timeit.default_timer()\n\n# Fit the trained model on our data\nsel_model.fit(x, y)\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) \n\n# get index of good features\nsel_index = sel_model.get_support()\n\n# count the no of columns selected\ncounter = collections.Counter(sel_model.get_support())\ncounter","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.570608Z","iopub.execute_input":"2021-06-25T22:09:58.570889Z","iopub.status.idle":"2021-06-25T22:09:58.623567Z","shell.execute_reply.started":"2021-06-25T22:09:58.570863Z","shell.execute_reply":"2021-06-25T22:09:58.622498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_prof_ind_df = pd.DataFrame(x[x.columns[(sel_index)]])\nuser_profile_images_final = pd.concat([user_vectorized_profile_images[['user_id']], user_prof_ind_df], axis=1)\nuser_profile_images_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.624766Z","iopub.execute_input":"2021-06-25T22:09:58.625117Z","iopub.status.idle":"2021-06-25T22:09:58.656725Z","shell.execute_reply.started":"2021-06-25T22:09:58.625085Z","shell.execute_reply":"2021-06-25T22:09:58.655735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Join all tables together","metadata":{}},{"cell_type":"code","source":"print(\"Shape:\")\nprint(\"train_tweets:\", train_tweets_final.shape)\nprint(\"train_tweets_media:\", train_tweets_media_final.shape) # join on tweet id\nprint(\"train_tweets_text:\", train_tweets_text_final.shape) # join on tweet id\nprint(\"\")\nprint(\"user\", users_final.shape) \nprint(\"user_description\", user_descriptions_final.shape) # join on user id\nprint(\"user_profile\", user_profile_images_final.shape) # join on user id","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.658069Z","iopub.execute_input":"2021-06-25T22:09:58.658469Z","iopub.status.idle":"2021-06-25T22:09:58.668269Z","shell.execute_reply.started":"2021-06-25T22:09:58.658427Z","shell.execute_reply":"2021-06-25T22:09:58.666985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tweets_vectorized_text and user_vectorized_profile_images has same column names. \n# rename columns in tweets_vectorized_text\ncols = train_tweets_text_final.columns[train_tweets_text_final.columns.str.contains('feature_')]\ntrain_tweets_text_final.rename(columns = dict(zip(cols, 'text_' + cols)), inplace=True)\ntrain_tweets_text_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.669885Z","iopub.execute_input":"2021-06-25T22:09:58.670525Z","iopub.status.idle":"2021-06-25T22:09:58.711Z","shell.execute_reply.started":"2021-06-25T22:09:58.67048Z","shell.execute_reply":"2021-06-25T22:09:58.709799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group media by tweet_id (since there are multiple media id for a single tweet)\nmedia_df = train_tweets_media_final.groupby('tweet_id').mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:58.712471Z","iopub.execute_input":"2021-06-25T22:09:58.712891Z","iopub.status.idle":"2021-06-25T22:09:59.86537Z","shell.execute_reply.started":"2021-06-25T22:09:58.712847Z","shell.execute_reply":"2021-06-25T22:09:59.864518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tweets_vectorized_text and user_vectorized_profile_images has same column names. \n# rename columns in tweets_vectorized_text\ncols = train_tweets_text_final.columns[train_tweets_text_final.columns.str.contains('feature_')]\ntrain_tweets_text_final.rename(columns = dict(zip(cols, 'text_' + cols)), inplace=True)\ntrain_tweets_text_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:59.86674Z","iopub.execute_input":"2021-06-25T22:09:59.867337Z","iopub.status.idle":"2021-06-25T22:09:59.899722Z","shell.execute_reply.started":"2021-06-25T22:09:59.867284Z","shell.execute_reply":"2021-06-25T22:09:59.898438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge all tables on the column 'user_id' for user data and tweet_id for tweet data\n\n# Join tweets data\ntweet_df = pd.merge(media_df, train_tweets_text_final, on = 'tweet_id', how = 'right')\ntweet_df.fillna(0, inplace=True)\n\n# Join users data\nuser_df = pd.merge(users_final, user_profile_images_final, on='user_id')\n\n# Join tweets data on train_tweets\ntweet_df_final = pd.merge(train_tweets_final, tweet_df, on = 'tweet_id')\n\n# Join with the users data\nfinal_df = pd.merge(tweet_df_final, user_df, left_on = 'tweet_user_id', right_on='user_id')\n\nfinal_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:09:59.901051Z","iopub.execute_input":"2021-06-25T22:09:59.901363Z","iopub.status.idle":"2021-06-25T22:10:02.538347Z","shell.execute_reply.started":"2021-06-25T22:09:59.901331Z","shell.execute_reply":"2021-06-25T22:10:02.537268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Test Data\nThe preprocessing done on the train data is replicated on the test data, so that our model we train using our train data is usable for our test data.\n## Test Tweets\n### Missing Values","metadata":{}},{"cell_type":"code","source":"test_tweets.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:02.53965Z","iopub.execute_input":"2021-06-25T22:10:02.539994Z","iopub.status.idle":"2021-06-25T22:10:02.552165Z","shell.execute_reply.started":"2021-06-25T22:10:02.539964Z","shell.execute_reply":"2021-06-25T22:10:02.551214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill missing values as done in Train Tweets\ntest_tweets.fillna({'tweet_topic_ids':\"['0']\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:02.553777Z","iopub.execute_input":"2021-06-25T22:10:02.554123Z","iopub.status.idle":"2021-06-25T22:10:02.569553Z","shell.execute_reply.started":"2021-06-25T22:10:02.554094Z","shell.execute_reply":"2021-06-25T22:10:02.568531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding","metadata":{}},{"cell_type":"code","source":"# One hot Encoding\ntopic_ids = (\n    test_tweets['tweet_topic_ids'].str.strip('[]').str.split('\\s*,\\s*').explode()\n    .str.get_dummies().sum(level=0).add_prefix('topic_id_')\n) \ntopic_ids.rename(columns = lambda x: x.replace(\"'\", \"\"), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:02.570668Z","iopub.execute_input":"2021-06-25T22:10:02.570974Z","iopub.status.idle":"2021-06-25T22:10:08.073667Z","shell.execute_reply.started":"2021-06-25T22:10:02.570946Z","shell.execute_reply":"2021-06-25T22:10:08.072752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year = pd.get_dummies(test_tweets.tweet_created_at_year, prefix='year')\nmonth = pd.get_dummies(test_tweets.tweet_created_at_month , prefix='month')\nday = pd.get_dummies(test_tweets.tweet_created_at_day, prefix='day')\nattachment = pd.get_dummies(test_tweets.tweet_attachment_class, prefix='attatchment')\nlanguage = pd.get_dummies(test_tweets.tweet_language_id, prefix='language')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.075031Z","iopub.execute_input":"2021-06-25T22:10:08.075314Z","iopub.status.idle":"2021-06-25T22:10:08.086572Z","shell.execute_reply.started":"2021-06-25T22:10:08.075287Z","shell.execute_reply":"2021-06-25T22:10:08.085801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cyclical encoding\nhour_sin = np.sin(2*np.pi*test_tweets['tweet_created_at_hour']/24.0)\nhour_sin.name = 'hour_sin'\nhour_cos = np.cos(2*np.pi*test_tweets['tweet_created_at_hour']/24.0)\nhour_cos.name = 'hour_cos'","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.087542Z","iopub.execute_input":"2021-06-25T22:10:08.087855Z","iopub.status.idle":"2021-06-25T22:10:08.103456Z","shell.execute_reply.started":"2021-06-25T22:10:08.087825Z","shell.execute_reply":"2021-06-25T22:10:08.102365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_drop = [\n                \"tweet_topic_ids\",\n                \"tweet_created_at_year\",\n                \"tweet_created_at_month\",\n                \"tweet_created_at_day\",\n                \"tweet_attachment_class\",\n                \"tweet_language_id\",\n                \"tweet_created_at_hour\",\n              ]\ndfs = [\n        topic_ids,\n        year,\n        month,\n        day,\n        attachment,\n        language,\n        hour_sin,\n        hour_cos,\n      ]\n\ntest_tweets_final = test_tweets.drop(columns_drop, 1).join(dfs)\ntest_tweets_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.104853Z","iopub.execute_input":"2021-06-25T22:10:08.105146Z","iopub.status.idle":"2021-06-25T22:10:08.147442Z","shell.execute_reply.started":"2021-06-25T22:10:08.105118Z","shell.execute_reply":"2021-06-25T22:10:08.146537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Columns","metadata":{}},{"cell_type":"code","source":"# Columns missing in train from test\ncols_test = set(test_tweets_final.columns) - set(train_tweets_final.columns)\ncols_test","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.148645Z","iopub.execute_input":"2021-06-25T22:10:08.148936Z","iopub.status.idle":"2021-06-25T22:10:08.156666Z","shell.execute_reply.started":"2021-06-25T22:10:08.148908Z","shell.execute_reply":"2021-06-25T22:10:08.155731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols_test:\n    final_df[col] = 0","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.157883Z","iopub.execute_input":"2021-06-25T22:10:08.158186Z","iopub.status.idle":"2021-06-25T22:10:08.174024Z","shell.execute_reply.started":"2021-06-25T22:10:08.158158Z","shell.execute_reply":"2021-06-25T22:10:08.172818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns missing in test from train\ncols_train = set(train_tweets_final.columns) - set(test_tweets_final.columns)\ncols_train.remove('virality') # remove virality from columns to add to test\ncols_train","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.175242Z","iopub.execute_input":"2021-06-25T22:10:08.175539Z","iopub.status.idle":"2021-06-25T22:10:08.189854Z","shell.execute_reply.started":"2021-06-25T22:10:08.175511Z","shell.execute_reply":"2021-06-25T22:10:08.188845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols_train:\n    test_tweets_final[col] = 0","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.191215Z","iopub.execute_input":"2021-06-25T22:10:08.191752Z","iopub.status.idle":"2021-06-25T22:10:08.208221Z","shell.execute_reply.started":"2021-06-25T22:10:08.191719Z","shell.execute_reply":"2021-06-25T22:10:08.207314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Join data","metadata":{}},{"cell_type":"code","source":"test_tweets_media_final = pd.concat([test_tweets_vectorized_media[['media_id', 'tweet_id']], media_ind_df], axis=1)\ntest_tweets_text_final = pd.concat([test_tweets_vectorized_text[['tweet_id']], text_ind_df], axis=1)\n\nmedia_df = test_tweets_media_final.groupby('tweet_id').mean()\n\ncols = test_tweets_text_final.columns[test_tweets_text_final.columns.str.contains('feature_')]\ntest_tweets_text_final.rename(columns = dict(zip(cols, 'text_' + cols)), inplace=True)\n\n# Join tweets data\ntweet_df = pd.merge(media_df, test_tweets_text_final, on = 'tweet_id', how = 'right')\ntweet_df.fillna(0, inplace=True)\n\n# Join users data\nuser_df = pd.merge(users_final, user_profile_images_final, on='user_id')\n\n# Join tweets data on train_tweets\ntweet_df_final = pd.merge(test_tweets_final, tweet_df, on = 'tweet_id')\n\n# Join with user data\np_final_df = pd.merge(tweet_df_final, user_df, left_on = 'tweet_user_id', right_on='user_id')\n\np_final_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:08.209419Z","iopub.execute_input":"2021-06-25T22:10:08.209765Z","iopub.status.idle":"2021-06-25T22:10:11.359344Z","shell.execute_reply.started":"2021-06-25T22:10:08.20973Z","shell.execute_reply":"2021-06-25T22:10:11.358269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.shape ","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:11.360606Z","iopub.execute_input":"2021-06-25T22:10:11.360913Z","iopub.status.idle":"2021-06-25T22:10:11.366942Z","shell.execute_reply.started":"2021-06-25T22:10:11.360884Z","shell.execute_reply":"2021-06-25T22:10:11.365862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train has one more column than test because of virality column","metadata":{}},{"cell_type":"markdown","source":"# Memory Footprint reduction.\nFunction takes the minimum and the maximum of each column and changes the data type to what is optimal for the column. Implementation copied from [Eryk Lewson](https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4)","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_df = reduce_mem_usage(pd.read_csv(\"../input/temp-twitter-virality/final_df.csv\"))\np_final_df = reduce_mem_usage(pd.read_csv(\"../input/temp-twitter-virality/p_final_df.csv\"))\nprint(\"Shape of train set: \", final_df.shape)\nprint(\"Shape of test set: \", p_final_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:10:11.368266Z","iopub.execute_input":"2021-06-25T22:10:11.368616Z","iopub.status.idle":"2021-06-25T22:17:49.586196Z","shell.execute_reply.started":"2021-06-25T22:10:11.368585Z","shell.execute_reply":"2021-06-25T22:17:49.5851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model fitting\n## Split the full sample into train/test (70/30)","metadata":{}},{"cell_type":"code","source":"X = final_df.drop(['virality', 'tweet_user_id', 'tweet_id', 'user_id'], axis=1)\ny = final_df['virality']\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)\nprint('Training set shape ', X_train.shape)\nprint('Test set shape ', X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:17:49.587562Z","iopub.execute_input":"2021-06-25T22:17:49.587881Z","iopub.status.idle":"2021-06-25T22:17:50.544814Z","shell.execute_reply.started":"2021-06-25T22:17:49.58785Z","shell.execute_reply":"2021-06-25T22:17:50.543785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# param_test = {'num_leaves': sp_randint(6, 50), \n#             'min_child_samples': sp_randint(100, 500), \n#             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n#             'subsample': sp_uniform(loc=0.2, scale=0.8), \n#             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n#             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n#             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\n#clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n#gs = RandomizedSearchCV(\n#    estimator=clf, param_distributions=param_test, \n#    n_iter=100,\n#    scoring= 'f1_macro',\n#    cv=3,\n#    refit=True,\n#    random_state=314,\n#    verbose=True)\n\n#gs.fit(X_train, y_train, **fit_params)\n#print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:17:50.548226Z","iopub.execute_input":"2021-06-25T22:17:50.54855Z","iopub.status.idle":"2021-06-25T22:17:50.555917Z","shell.execute_reply.started":"2021-06-25T22:17:50.548519Z","shell.execute_reply":"2021-06-25T22:17:50.554843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best score reached: 0.48236216974224616 with params: {'colsample_bytree': 0.7076074093370144, 'min_child_samples': 105, 'min_child_weight': 1e-05, 'num_leaves': 26, 'reg_alpha': 5, 'reg_lambda': 5, 'subsample': 0.7468773130235173} ","metadata":{"execution":{"iopub.status.busy":"2021-06-27T16:59:34.845708Z","iopub.execute_input":"2021-06-27T16:59:34.846017Z","iopub.status.idle":"2021-06-27T16:59:34.857031Z","shell.execute_reply.started":"2021-06-27T16:59:34.845947Z","shell.execute_reply":"2021-06-27T16:59:34.855554Z"}}},{"cell_type":"code","source":"opt_params = {'num_leaves': 26,\n             'min_child_samples': 105,\n             'min_child_weight': 1e-05,\n             'subsample': 0.7468773130235173,\n             'colsample_bytree': 0.7076074093370144,\n             'reg_alpha': 5,\n             'reg_lambda': 5\n             }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = lgb.LGBMClassifier(**opt_params)\nclf.fit(\n    X_train, y_train, \n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    early_stopping_rounds=10\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction on the test dataset\ny_pred = clf.predict(X_test)\n\n# Base accuracy 66.45%\n# 0.6849 LGBMClassifier(max_depth=12, num_leaves=300)\nprint('Accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Identify feature importance","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:10], color='blue')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:17:51.034561Z","iopub.status.idle":"2021-06-25T22:17:51.035277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(clf)\nshap_values = explainer.shap_values(X)\nshap.summary_plot(shap_values, X)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:17:51.036382Z","iopub.status.idle":"2021-06-25T22:17:51.036819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Density scatter plot of SHAP values for each feature identifies how much impact each feature has on the model output for individuals in the validation dataset.  \nClass 0 corresponds to Virality 1 and so forth.","metadata":{"execution":{"iopub.status.busy":"2021-06-24T02:39:54.766376Z","iopub.execute_input":"2021-06-24T02:39:54.766728Z","iopub.status.idle":"2021-06-24T02:39:54.774228Z","shell.execute_reply.started":"2021-06-24T02:39:54.766699Z","shell.execute_reply":"2021-06-24T02:39:54.772355Z"}}},{"cell_type":"markdown","source":"# Fit model to Test data","metadata":{}},{"cell_type":"code","source":"X = p_final_df.drop(['tweet_user_id', 'tweet_id', 'user_id'], axis=1)\n\nsolution = clf.predict(X)\nsolution_df = pd.concat([p_final_df[['tweet_id']], pd.DataFrame(solution, columns = ['virality'])], axis=1)\nsolution_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:17:51.037629Z","iopub.status.idle":"2021-06-25T22:17:51.038119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#solutions_format = pd.read_csv(\"../input/viral-tweets/Dataset/solution_format.csv\")\nsolutions_format = solutions_format.drop([\"virality\"], axis=1)\nfinal_solution = solutions_format.merge(solution_df, left_on='tweet_id', right_on='tweet_id')\nfinal_solution","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:17:51.038998Z","iopub.status.idle":"2021-06-25T22:17:51.03945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_solution.to_csv(\"final_solution.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T22:17:51.040763Z","iopub.status.idle":"2021-06-25T22:17:51.041201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next Steps\n- More feature engineering\n- Further parameter tuning\n- Stacking ensemble ML models\n- Learning rate decay in LightGBM model training to improve convergence to the minimum","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}